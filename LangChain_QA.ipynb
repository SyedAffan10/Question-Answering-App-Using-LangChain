{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8559dfd-b5b9-46bd-910a-57bc15765242",
   "metadata": {},
   "source": [
    "# LangChain QA\n",
    "\n",
    "All code comes from [LangChain docs](langchain.readthedocs.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7688495-ef79-4831-95bc-8c77eeb9b97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community openai chromadb tiktoken pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4574ffb7-53da-480e-bf82-46d9d794ce82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4793f8d6-bf79-4513-8a31-06e209852a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ba580-2c29-450b-bb9c-edd301a7da4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "print(llm(\"tell me a joke\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f34b4-b121-4c9a-8425-7a8fa8fea367",
   "metadata": {},
   "source": [
    "# load_qa_chain\n",
    "\n",
    "Loads a chain that you can use to do QA over a set of documents, but it uses ALL of those documents. \n",
    "\n",
    "chain_type=\"stuff\" will not work because the number of tokens exceeds the limit. We can try other chain types like \"map_reduce\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343c502-093d-4161-9f14-9fc52a8b725c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "\n",
    "### Load one document at a time\n",
    "# loader = PyPDFLoader(\"PDF/ai1.pdf\")\n",
    "# documents = loader.load()\n",
    "\n",
    "### Load multiple documents at a time\n",
    "# loaders = [\"PDF/ai1.pdf\", \"PDF/ai2.pdf\"]\n",
    "# documents = []\n",
    "# for loader in loaders:\n",
    "#     documents.extend(loader.load())\n",
    "\n",
    "### Load multiple documents from a directory\n",
    "directory_path = \"PDF/\"\n",
    "loader = PyPDFDirectoryLoader(\"PDF/\")\n",
    "documents = loader.load()\n",
    "\n",
    "chain = load_qa_chain(llm=OpenAI(), chain_type=\"map_reduce\")\n",
    "query = \"what is Artificial intelligence?\"\n",
    "chain.run(input_documents=documents, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca2b73-bdae-4dac-bb75-bf10cabc3a66",
   "metadata": {},
   "source": [
    "# RetrievalQA \n",
    "\n",
    "RetrievalQA chain uses load_qa_chain under the hood. We retrieve the most relevant chunck of text and feed those to the language model. \n",
    "\n",
    "\n",
    "#### Options: \n",
    "- [embeddings](https://python.langchain.com/v0.2/docs/how_to/#embedding-models)\n",
    "- [TextSplitter](https://python.langchain.com/v0.2/docs/how_to/#text-splitters)\n",
    "- [VectorStore](https://python.langchain.com/v0.2/docs/how_to/#vector-stores)\n",
    "- [Retrievers](https://python.langchain.com/v0.2/docs/how_to/#retrievers)\n",
    "  - [search_type](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/#similarity-search): \"similarity\" or \"mmr\"\n",
    "- [Chain Type](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain.html#langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain): \"stuff\", \"map reduce\", \"refine\", \"map_rerank\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d4a44-da40-48f7-b5b6-ab503d0afa3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load document\n",
    "loader = PyPDFLoader(\"PDF/ai1.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# create the vectorestore to use as the index\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "query = \"what is Artificial intelligence?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6d12f-484c-480f-a814-b2823386af4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d94bb-77fd-4b2c-9440-0eb968d89181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ffbdf-a0c1-4da7-9080-e7d7e47b57d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VectorstoreIndexCreator\n",
    "\n",
    "VectorstoreIndexCreator is a wrapper for the above logic. \n",
    "\n",
    "Source: \n",
    "- https://python.langchain.com/v0.1/docs/modules/chains/\n",
    "- https://python.langchain.com/v0.2/docs/how_to/#vector-stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d85b40-37aa-43b0-a29e-cda49f972425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    # split the documents into chunks\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0),\n",
    "\n",
    "    # select which embeddings we want to use\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    \n",
    "    # use Chroma as the vectorestore to index and search embeddings\n",
    "    vectorstore_cls=Chroma\n",
    ").from_loaders([loader])\n",
    "query = \"what is Artificial intelligence?\"\n",
    "index.query(llm=OpenAI(), question=query, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a2232-52f8-4564-b489-314db1fc2506",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ConversationalRetrievalChain\n",
    "\n",
    "conversation memory + RetrievalQAChain\n",
    "\n",
    "Allow for passing in chat history which can be used for follow up questions.\n",
    "\n",
    "Source: https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68660c38-8cb3-47de-8270-52123481f018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501fc12-da80-4030-b31d-58090f8f9df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load document\n",
    "loader = PyPDFLoader(\"PDF/ai1.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# create the vectorestore to use as the index\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(), retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645adf8-6639-45ff-b0f0-bfd9a7cae631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"what is Artificial intelligence?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a317b79-fd4f-455b-a5b0-d767cdcdd700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ce166-97a2-48d2-96c5-9eb5bdb9ad9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"What is machine learning?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f226467-7fbb-43bc-8a5b-acf484f29126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d08c88-1f76-4e91-b886-a720aee8aca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c915d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
